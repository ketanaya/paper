\documentclass[uplatex,a4j,11pt,dvipdfmx]{ujreport}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{mdframed}
\usepackage{amsmath}
\makeatletter
\def\kcharparline#1{
    \newcounter{chr}
    \setcounter{chr}{#1}
    \@tempdima=\linewidth
    \advance\@tempdima by-\value{chr}zw
    \addtocounter{chr}{-1}
    \divide\@tempdima by \value{chr}
    \advance\kanjiskip by\@tempdima
    \advance\parindent by\@tempdima}
\def\lineparpage#1{
    \baselineskip=\textheight
    \divide\baselineskip by #1}
\makeatother


\renewcommand{\bibname}{参考文献}

\begin{document}

\kcharparline{40} % 一行の文字数
\lineparpage{25}  % 一頁の行数


\title{卒業論文 \\
日経平均株価の時系列分析}
\author{
関西学院大学 商学部 \\
26022435 宮崎綾平% <--ここに学生番号と氏名を入れる
}
\date{2025年12月02日} % <--ここに作成日を入れる. 
%\date{\today} % 


\maketitle % タイトルページの作成

\tableofcontents % 目次の作成

\chapter{はじめに}

()

\chapter{データに関する検証} %<--ここに章のタイトルを入れる
この章では、2015年から2024年までの過去10年分の日経平均株価のデータを取得し、時系列データの図示を行うことによって時系列データの
大まかな特徴を捉え、どのような解析を行うべきか方針をたてる。
\section{データの取得} % <--ここに節のタイトルを入れる
今回分析ではRのquantmodパッケージを使って下記のように記述することで、2015年から2024年の日経平均株価のデータを取得している。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> nikkei <- getSymbols(Symbols = "^N225", 
                       src = "yahoo",
                       from = "2015-01-01",
                       to = "2024-12-31",
                       auto.assign = FALSE)
\end{Verbatim}

\section{データの前処理} \label{sec:maesyori}
この章では分析を行うためのデータの前処理を行う。まずはデータの列構造などについてhead関数を用いて確認する。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> head(nikkei)
           N225.Open N225.High N225.Low N225.Close N225.Volume N225.Adjusted
2015-01-05  17325.68  17540.92 17219.22   17408.71   116500000      17408.71
2015-01-06  17101.58  17111.36 16881.73   16883.19   166000000      16883.19
2015-01-07  16808.26  16974.61 16808.26   16885.33   138600000      16885.33
2015-01-08  17067.40  17243.71 17016.09   17167.10   140600000      17167.10
2015-01-09  17318.74  17342.65 17129.53   17197.73   155200000      17197.73
2015-01-13  16970.88  17087.71 16828.27   17087.71   149200000      17087.71
\end{Verbatim}

このデータには日付に加え、「N225.Open,N225.High,N225.Low,N225.Close,N225.Volume,\\N225.Adjusted」という列が存在し、それぞれ
「始値、高値、安値、終値、出来高、調整後終値」を意味する。今回の分析では、終値が当日の市場取引が全て完結した時点の値であるという観点から、
終値を分析対象とする。次に終値の列だけを抽出し、summary関数を用いてデータの概要を確認する。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> nikkei_cl <- Cl(nikkei)
> summary(nikkei_cl)
     Index              N225.Close   
 Min.   :2015-01-05   Min.   :14952  
 1st Qu.:2017-07-07   1st Qu.:20013  
 Median :2019-12-14   Median :22823  
 Mean   :2019-12-27   Mean   :24832  
 3rd Qu.:2022-06-28   3rd Qu.:28451  
 Max.   :2024-12-30   Max.   :42224  
                      NA's   :21           
\end{Verbatim}

上記の結果から、今回のデータでは終値は14952円から42224円の間で推移していることがわかった。
また、データが21個の欠測値を持つことがわかったため、これらの欠測値について検証する。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> na <- nikkei_cl[is.na(nikkei_cl)]
> na
           N225.Close
2017-07-17         NA
2017-08-11         NA
2017-09-18         NA
2017-10-09         NA
2017-11-03         NA
2017-11-23         NA
2018-01-01         NA
2018-01-02         NA
2018-01-03         NA
2018-01-08         NA
2018-02-12         NA
2018-03-21         NA
2018-04-30         NA
2018-05-03         NA
2018-05-04         NA
2018-09-17         NA
2018-09-24         NA
2018-10-08         NA
2018-11-23         NA
2018-12-24         NA
2018-12-31         NA
> nikkei_cl <- na.omit(nikkei_cl)
\end{Verbatim}

上記の結果をもとに、欠測値が記録されている日付について調べると、全て祝日であることがわかった。
東京証券取引所は祝日は取引を行っていないため、これらの欠測値を含んだ列は分析に不要と考え、na.omit関数によってデータから取り除く処理を行う。

\section{時系列データの図示} \label{sec:plot}
\ref{sec:maesyori}節で前処理を行った時系列データを用い、観測地点を横軸に、観測値を縦軸にとり隣り合う観測値を直線で結んだ図を描く。
この図を描くことで、時系列データが\emph{トレンド}や\emph{季節性}を含むかをどうかを可視化することができる。なお、トレンドと季節性について定義を
\cite{Hyndman}から引用する。
\begin{itemize}
    \item トレンド
    \begin{itemize}
    \item A trend exists when there is a long-term increase or decrease in the data.
     It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, 
     when it might go from an increasing trend to a decreasing trend. 
    \end{itemize}
\end{itemize}    

\begin{itemize}
    \item 季節性
    \begin{itemize}
    \item A seasonal pattern occurs when a time series is affected by seasonal factors such as 
    the time of the year or the day of the week. 
    Seasonality is always of a fixed and known period. 
    \end{itemize}
\end{itemize}   

これらを踏まえた上で、時系列データの図示を行ったところ、図 \ref{fig:nikkei-cl} のような結果が得られた。
この図からこのデータは上昇トレンドがあることが見て取れる。また、季節性に関しては図を見る限り、確認することはできない。
加えて、値が急激に変化している地点についても調査を行った。2020年の2月から3月にかけて値の急激な減少の後に、急上昇をしている地点が存在する。
この原因はコロナによる経済活動の停滞及び、それを受けた日銀の金融政策に対する期待である。また、2024年の1月頃に値が急上昇している地点がある。
この原因はNVIDIAが市場を上回る好決算を発表したことで、東京市場の半導体関連株が買われたことによるものである。
なお、これらは\cite{nikkei-1},\cite{nikkei-2}を参照している。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/nikkei_plot}
\caption{日経平均株価(終値)の推移} 
\label{fig:nikkei-cl} 
\end{figure}

\section{時系列データの自己相関}

図 \ref{fig:acf} は、データの自己相関をプロットしたものである。詳細な自己相関係数についても確認できる。(ここに考察)

\begin{figure}[htbp] %何故か図がコードブロックに入り込んでる
    \centering
    \includegraphics[width=0.9\linewidth]{figures/acf}
    \caption{日経平均株価(終値)の自己相関} 
    \label{fig:acf} 
    \end{figure}

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> acf(nikkei_cl, plot = FALSE)
Autocorrelations of series ‘nikkei_cl’, by lag

    0     1     2     3     4     5     6     7     8     9    10    11    12 
1.000 0.997 0.994 0.992 0.989 0.986 0.984 0.981 0.979 0.976 0.974 0.972 0.969 
   13    14    15    16    17    18    19    20    21    22    23    24    25 
0.967 0.964 0.962 0.960 0.957 0.955 0.953 0.951 0.949 0.947 0.945 0.943 0.941 
   26    27    28    29    30    31    32    33 
0.939 0.937 0.935 0.933 0.931 0.929 0.927 0.924 
\end{Verbatim}

\chapter{時系列データの分解}

\section{decomp関数の概要} \label{sec:decomp}
この章では\ref{sec:plot}節で得られた、データが上昇トレンドを持っているという考察を、Rのtimsacパッケージに含まれるdecomp関数を使って
検証する。decomp関数とは時系列データを下記の\ref{model:decomp}式に当てはめ、トレンド成分や季節成分などのいくつかの成分に分解する関数である。

\begin{equation} \label{model:decomp}
y(t) = T(t) + AR(t) + S(t) + TD(t) + W(t)
\end{equation} 

ここで、T(t)はトレンド成分、AR(t)はAR成分、S(t)は季節成分、TD(t)は曜日効果成分、W(t)は観測ノイズである。
なお, これらは \cite{timsac} を参照している。\\
decomp関数を用いるためには各成分の次数を適切に設定し、当てはまりの良いモデルを作成する必要がある。
よって、次にモデルの次数選択を行う。まず、図\ref{fig:nikkei-cl}から今回のデータは季節成分及び、曜日効果成分を持たないと考え、除外する。
次にトレンド成分、AR成分の次数を決定するために、AIC(赤池情報量基準, Akaike's Information Criterion)を用いる。(AICの説明)
Rで下記のように記述することで、トレンド成分の次数を1,2,3、AR成分の次数を1~7に設定し、全ての組み合わせのAICを求めた。
これらの組み合わせの中で最もAICが小さいモデルを採用し、decomp関数を実行する。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
    > library(timsac)
    > y <- nikkei_cl
    > 
    > for (m1 in 1:3) {
    +   for (m2 in 1:7) {
    +     k <- 0
    +     
    +     fit <- try(
    +       decomp(
    +         y,
    +         trend.order    = m1,
    +         ar.order       = m2,
    +         seasonal.order = k,
    +         plot           = FALSE
    +       ),
    +       silent = TRUE
    +     )
    +     
    +     if (inherits(fit, "try-error")) {
    +       cat("[ERROR]",
    +           "trend.order =", m1,
    +           "ar.order =", m2,
    +           "seasonal.order =", k,
    +           "AIC = NA\n")
    +     } else {
    +       cat("trend.order =", m1,
    +           "ar.order =", m2,
    +           "seasonal.order =", k,
    +           "AIC =", fit$aic, "\n")
    +     }
    +   }
    + }    
\end{Verbatim}
上記を実行すると下記のような結果が得られる。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
    trend.order = 1 ar.order = 1 seasonal.order = 0 AIC = 35465.07 
    trend.order = 1 ar.order = 2 seasonal.order = 0 AIC = 35370.69 
    trend.order = 1 ar.order = 3 seasonal.order = 0 AIC = 35369.54 
    trend.order = 1 ar.order = 4 seasonal.order = 0 AIC = 35368.89 
    trend.order = 1 ar.order = 5 seasonal.order = 0 AIC = 35631.88 
    trend.order = 1 ar.order = 6 seasonal.order = 0 AIC = 35354.22 
    trend.order = 1 ar.order = 7 seasonal.order = 0 AIC = 35388.21 
    trend.order = 2 ar.order = 1 seasonal.order = 0 AIC = 35644.09 
    trend.order = 2 ar.order = 2 seasonal.order = 0 AIC = 35411.51 
    trend.order = 2 ar.order = 3 seasonal.order = 0 AIC = 35402.68 
    trend.order = 2 ar.order = 4 seasonal.order = 0 AIC = 35389.37 
    trend.order = 2 ar.order = 5 seasonal.order = 0 AIC = 35393.69 
    trend.order = 2 ar.order = 6 seasonal.order = 0 AIC = 35390.23 
    trend.order = 2 ar.order = 7 seasonal.order = 0 AIC = 35391.92 
    trend.order = 3 ar.order = 1 seasonal.order = 0 AIC = 35832.37 
    trend.order = 3 ar.order = 2 seasonal.order = 0 AIC = 35560.81 
    trend.order = 3 ar.order = 3 seasonal.order = 0 AIC = 35550.06 
    trend.order = 3 ar.order = 4 seasonal.order = 0 AIC = 35530.52 
    trend.order = 3 ar.order = 5 seasonal.order = 0 AIC = 35534.47 
    trend.order = 3 ar.order = 6 seasonal.order = 0 AIC = 35527.38 
    trend.order = 3 ar.order = 7 seasonal.order = 0 AIC = 35530.68    
\end{Verbatim}
この結果から、トレンド次数 = 1, AR次数 = 6の組み合わせがAIC = 35354.22で最小になった。よって、このモデルを採用し、decomp
関数を実行する。

\section{decomp関数の実行} \label{sec:decomp-result}

\ref{sec:decomp}節で決定したモデルを用いて、下記のように記述し、時系列データの分解を行う。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> dp <- decomp(nikkei_cl, trend.order = 1, ar.order = 6, seasonal.order = 0)
\end{Verbatim}
上記を実行すると図\ref{fig:decomp}ような結果が得られた。この結果から、時系列データがトレンド成分を持つことが明らかになった。
また、ノイズに関してもほとんど見られないためモデルの当てはまりは良いと考えられる。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/decomp_plot.png}
    \caption{時系列データの分解} 
    \label{fig:decomp} 
    \end{figure}



\chapter{ARIMAモデルによる予測}
\section{定常性の確認} \label{sec:teizyou}
この章では過去10年分の日経平均株価の時系列データを用いて、今後の変動を予測する。予測を行うには時系列データの特徴を踏まえ、適切な時系列モデルを選択
する必要がある。時系列モデルの選択を行う上で重要なのは、時系列データの\emph{定常性}を確認することである。ここで、定常性について定義を行う。
\begin{equation}
lを時間のシフト量を表す任意の整数とするとき
\end{equation} 
また、
\begin{equation}
強定常の定義
\end{equation}
弱定常を定常性と呼ぶ。今回使用する時系列データは、\ref{sec:decomp-result}節で上昇トレンドを持っていることがわかっており、時点が異なると
値が異なるため非定常の時系列データである。

\section{ARIMAモデルの概要}
\ref{sec:teizyou}節で時系列データが非定常であることがわかった。非定常な時系列データに対しては様々なアプローチがあるが、今回はARIMAモデル
を使用し、将来の値を予測する。ここで、ARIMAモデルについての定義を行う。そのために必要な\emph{差分}、
\emph{自己回帰モデル}、\emph{移動平均モデル}について説明する。なお, これらは \cite{Hyndman} を参照している。

\subsection{差分}
非定常な時系列を定常にする方法の1つに、連続する観測値の差分を計算する方法がある。これを差分を取ると言う。差分を取ることは
時系列のトレンドや季節性を除去、あるいは低減し、時系列の平均を安定化させることに役立つ。
ここで、差分系列(The differenced series)は元の系列の連続する観測値間の変化であり下記のように書くことができる。
\begin{equation} \label{model:diff}
y'_t = y_t - y_{t-1}
\end{equation} 
また、一度差分を取っても時系列が定常にならないときはは2次差分を取る必要がある場合が存在する。
\begin{align}
\begin{split} \label{model:doff-second}
    y''_t &= y'_t - y'_{t-1} \\
          &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) \\
          &= y_t - 2y_{t-1} + y_{t-2}
\end{split}
\end{align}
なお、実際の分析では2次よりも高次の差分を取ることは必要ない場合が多い。

\subsection{自己回帰モデル}
下記のようなモデルを$p$次の自己回帰モデル(autoregressive model)、$AR(p)$モデルと呼ぶ。
\begin{equation} \label{model:ar}
y_t = c + \phi_1y_{t-1} + \phi_2y_{t-2} + \dots + \phi_py_{t-p} + \varepsilon_t
\end{equation}
ただし、$\varepsilon_t$はホワイトノイズである。

\subsection{移動平均モデル}
下記のようなモデルを$q$次の移動平均モデル(moving average model)、$MA(q)$モデルと呼ぶ。
\begin{equation} \label{model:ma}
y_t = c + \varepsilon_t +\theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q} + 
\end{equation}
ただし、$\varepsilon_t$はホワイトノイズである。
\subsection{ARIMAモデル}
差文化、自己回帰モデル、移動平均モデルを組み合わせると、ARIMAモデル(AutoRegressive Integrated Moving Average)
となる。ARIMAモデルのモデル式は下記のように書くことができる。
\begin{equation} \label{model:arima}
y'_t = c + \phi_1y'_{t-1} + \dots + \phi_py'_{t-p} + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q} + \varepsilon_t
\end{equation} 
ただし$y'_t$は一回を超えて差分を取る場合もある。\ref{model:arima}式を$ARIMA(p,d,q)$モデルと呼ぶ。

\section{モデル化の手順} \label{sec:model}
次に、実際に日経平均株価の時系列データに対してARIMAモデルを当てはめていく。
時系列データにARIMAモデルを適合する際は、下記の手順を実施する。
\begin{enumerate}[1.]
    \item データをプロットして、異常値を特定する。
    \item 必要なら、(Box-Cox変換を使って)データを変換し、分散を安定化させる。
    \item データが非定常なら、定常になるまでデータの一つ目差分を取る。
    \item ACF/PACFを検討する: $ARIMA(p,d,0)$モデルと$ARIMA(0,d,q)$モデルのどちらが適切か？
    \item 選んだモデルを試す、そして、AICcを使ってより良いモデルを探索する。
    \item 選んだモデルからの残差を、ACFプロットやportmanteau検定でチェックする。残差がホワイトノイズらしくなければ、モデルを修正して試す。
    \item 残差がホワイトノイズらしく見えるようになったら、予測を計算する。
\end{enumerate}
なお, これらは \cite{Hyndman} を参照している。

\section{ARIMA()関数について} \label{sec:ARIMA()}
モデル化を行うにあたって、\ref{sec:model}節で示した、3〜5のステップをRのfableパッケージのARIMA()関数を用いて実施する。ARIMA()関数は後述する単位根検定とAICc最小化、そして最尤法を組み合わせて
ARIMAモデルを得る\textbf{Hyndman-Khandakarアルゴリズム}を使用している。
なお、Hyndman-Khandakarアルゴリズムについては\cite{Hyndman-and-Khandakar}を参照されたい。
ARIMA()関数は引数を指定することで、様々なアルゴリズムの変形を用いることができる。下記にHyndman-Khandakarアルゴリズムに従った、デフォルトでの
動作を記述する。
\begin{enumerate}[1.]
    \item 差分の次数 $0 \le d \le 2$ をKPSS検定を繰り返して決める。
    \item 次に、$d$ 回差分を取ったデータでAICcを最小化して、$p$ と $q$ の値を選ぶ。アルゴリズムは $p$ と $q$ の全ての可能な組み合わせを考えるのではなく、ステップワイズ探索を使ってモデル空間を横断する。
    \begin{enumerate}[a.]
        \item 最初に下記の4つのモデルに適合させる。
        \begin{itemize}
            \item $ARIMA(0,d,0)$
            \item $ARIMA(2,d,2)$
            \item $ARIMA(1,d,0)$
            \item $ARIMA(0,d,1)$
        \end{itemize}
        $d = 2$ でなければ定数を含める。$d \le 1$ なら、さらに追加で定数なしの $ARIMA(0,d,0)$ に適合させる。
        
        \item (a)のステップで適合させた中で（最小のAICc値を持つ）最良モデルを「現在のモデル」とする。
        
        \item 現在のモデルの変形を以下のようにして考える。
        \begin{itemize}
            \item 現在のモデルから、$\pm 1$ だけ $p$ と $q$ の両方か一方を変えてみる。
            \item 現在のモデルから、$c$ を取り除いたり、含めたりしてみる。
        \end{itemize}
        ここまでで最良と考えられるモデル（現在のモデルかこれらの変形の1つ）が新しい現在のモデルになる。
        
        \item (c)のステップをAICcがそれ以上小さくならなくなるまで繰り返す。
    \end{enumerate}
\end{enumerate}

\section{モデル作成} \label{sec:make-model}
\ref{sec:ARIMA()}を踏まえて、ARIMA()関数を実行し、モデル化を行う。Rで下記のように記述することでモデル化を行うことができる。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
    > nikkei_ts <- tibble(
+   date  = as.Date(time(nikkei_cl)),       
+   close = as.numeric(nikkei_cl)         
+ ) |> 
+   as_tsibble(index = date)
> 
> nikkei_ts_idx <- nikkei_ts |>
+   mutate(t = row_number()) |>
+   as_tsibble(index = t)
> 
> model <- nikkei_ts_idx |>
+   model(auto = ARIMA(close))
> 
> 
> report(model)
Series: close 
Model: ARIMA(2,1,1) w/ drift 

Coefficients:
          ar1     ar2     ma1  constant
      -0.7151  0.0121  0.6841   15.5509
s.e.   0.1616  0.0236  0.1605   11.1745

sigma^2 estimated as 107779:  log likelihood=-17626.22
AIC=35262.44   AICc=35262.47   BIC=35291.45
\end{Verbatim}
上記の結果から、アルゴリズムにより、$ARIMA(2,1,1)$モデルが選ばれたことがわかった。\ref{sec:model}節の手順6に従い、次節でこのモデルについての検証を行う。

\section{モデルの評価} \label{sec:model-check}
\ref{sec:model}節より、モデルの残差をACFプロット、及びportmanteau検定で確認する。

\subsection{ACFプロット}
Rで下記のように記述することで、ACFプロットを作成した。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
model |>
  gg_tsresiduals()
\end{Verbatim}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/acf_residual.png}
\caption{残差のACFプロット} 
\label{fig:acf-residual} 
\end{figure}

図\ref{fig:acf-residual}から残差は概ね、信頼区間に収まっており残差はホワイトノイズのようだと言うことができる。
\subsection{portmanteau検定}
Rで下記のように記述することで、portmanteau検定の一種であるLjung-Box検定を行った。なお、portmanteau検定及び、Ljung-Box検定については
を参照されたい。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 10, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      8.36     0.302
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 30, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      38.2    0.0747
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 20, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      25.2    0.0893
\end{Verbatim}
上記の結果から、ラグが10,20,30の場合でいずれもp値は0.05よりも多きいためモデルの残差はホワイトノイズであると言える。

\section{予測と考察}
\ref{sec:model-check}節でモデルの残差がホワイトノイズであることが確かめられたため、\ref{sec:model}節の手順7に移行し、将来の値の予測を行う。
なお、今回は1年分の予測を行うことを目標にし、東京証券取引所の1年間の営業日数約250日分の予測を行う。
下記のように記述して、予測を計算し、プロットする。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
model |>
  forecast(h = 250) |>
  filter(.model == "auto") |>
  autoplot(nikkei_ts_idx) +
  theme_minimal(base_family = "HiraginoSans-W3") +
  labs(
    x = "日付",
    y = "終\n値",
    level = "Prediction interval"
       ) + 
  theme(
    axis.title.y = element_text(angle = 0, vjust = 0.5),
    plot.background = element_rect(fill = "white", color = NA)
  )
\end{Verbatim}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/predict_plot.png}
\caption{日経平均株価(終値)の予測プロット} 
\label{fig:predict} 
\end{figure}

\chapter{おわりに} % <--ここに章のタイトルを入れる

()

\begin{thebibliography}{99} % 文献リストの作成

\bibitem{timsac}
The Institute of Statistical Mathematics (2023)
\textit{timsac: Time Series Analysis and Control Package},
URL: \url{https://cran.r-project.org/package=timsac}.

\bibitem{Hyndman} 
Hyndman, R.J., \& Athanasopoulos, G. (2021)
\textit{Forecasting: principles and practice, 3rd edition},
OTexts: Melbourne, Australia. 
URL: \url{https://otexts.com/fpp3/}. 
Accessed on 2 December 2025

\bibitem{Kitagawa} 北川源四郎 (2020) 
『Rによる時系列モデリング入門』,
岩波書店

\bibitem{nikkei-1} 
日本経済新聞, (10 April 2020),
『日経平均、3月は記録ずくめ新型コロナで変動大きく: 株式投資の超キホン日経平均を知ろう!(17)』,
URL: \url{https://www.nikkei.com/article/DGXMZO57807450Y0A400C2I00000/}

\bibitem{nikkei-2} 
日本経済新聞, (22 February 2024),
『日経平均、終値3万9098円　34年ぶり最高値更新』,
URL: \url{https://www.nikkei.com/article/DGXZQOFL220S70S4A220C2000000/}

\bibitem{Hyndman-and-Khandakar}
Hyndman, R. J., \& Khandakar, Y., (2008)
『Automatic time series forecasting: The forecast package for R. Journal of Statistical Software』,
URL: \url{https://www.jstatsoft.org/article/view/v027i03}

\bibitem{Janssens} Janssens, J. (2014) \textit{Data Science at the Command Line}, O'Reilly Media. 

(太田満久, 下田倫大, 増田泰彦監訳, 長尾高弘訳 (2015) 『コマンドラインではじめるデータサイエンス: 分析プロセスを自在に進めるテクニック』, オライリー・ジャパン.)

\bibitem{Jimichi2018-a} 地道正行 (2018-a)
『探索的財務ビッグデータ解析 --前処理、データラングリング、再現可能性--』, 
商学論究, 第66巻, 第1号, pp. 1-31, 
関西学院大学商学研究会. 

\bibitem{Jimichi2018-b} 地道正行 (2018-b) 
『探索的財務ビッグデータ解析 --データ可視化, 統計モデリング, モデル選択, モデル評価, 動的文書生成, 再現可能研究--』, 
商学論究, 第66巻, 第2号, pp. 1-41, 
関西学院大学商学研究会. 

\bibitem{Jimichi2018-c}
地道 正行 (2018-c)
『データサイエンスの基礎: R による統計学独習』, 
裳華房

\bibitem{Motohashi} 本橋智光 (2018)『前処理大全: データ分析のための SQL/R/Python 実践テクニック』, 技術評論社.

\bibitem{Nishida} 西田圭介 (2017)『ビッグデータを支える技術: 刻々とデータが脈打つ自動化の世界』, 技術評論社.

\bibitem{Tange} Tange, Ole, (2018) \textit{GNU Parallel 2018}, ISBN: 9781387509881, DOI: 10.5281/zenodo.1146014, URL: \url{https://doi.org/10.5281/zenodo.1146014}, Mar, 2018.

\bibitem{Wickham.et.al(2023)}
Wickham, H., M.~\c{C}etinkaya-Rundel, and G.~Grolemund, 
(2023)
\textit{R for Data Science: Import, Tidy, Transform, Visualize, and Model Data, 2nd edition}, 
O'Reilly. 

\end{thebibliography}

\section*{謝辞}

本研究に対して, 関学花子先生から適切なコメントを頂いたことに, 感謝の意を表する. 

\end{document}