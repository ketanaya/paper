\documentclass[uplatex,a4j,11pt,dvipdfmx]{ujreport}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage{fvextra}
\usepackage{mdframed}
\usepackage{amsmath}
\makeatletter
\def\kcharparline#1{
    \newcounter{chr}
    \setcounter{chr}{#1}
    \@tempdima=\linewidth
    \advance\@tempdima by-\value{chr}zw
    \addtocounter{chr}{-1}
    \divide\@tempdima by \value{chr}
    \advance\kanjiskip by\@tempdima
    \advance\parindent by\@tempdima}
\def\lineparpage#1{
    \baselineskip=\textheight
    \divide\baselineskip by #1}
\makeatother


\renewcommand{\bibname}{参考文献}

\begin{document}

\kcharparline{40} % 一行の文字数
\lineparpage{25}  % 一頁の行数


\title{卒業論文 \\
日経平均株価の時系列分析}
\author{
関西学院大学 商学部 \\
26022435 宮崎綾平% <--ここに学生番号と氏名を入れる
}
\date{2025年12月02日} % <--ここに作成日を入れる. 
%\date{\today} % 


\maketitle % タイトルページの作成

\tableofcontents % 目次の作成

\chapter{はじめに}

本研究では、日経平均株価を対象とし、時系列分析を用いた将来予測を行う。
近年、金融市場は拡大を続けており、日経平均株価もその例外ではない。2024年初頭には3万円台であった株価は、2025年現在では5万円台に到達するなど、上昇傾向にある。
しかし一方で、過去には新型コロナウイルスの感染拡大による世界的な大暴落が発生するなど、日経平均株価を含むあらゆる金融資産には常に下落リスクが内在している。

このように変動の激しい金融市場において、将来の価格変動を一定の精度で予測することは、投資判断やリスク管理などの意思決定を行う上で重要な意義を持つと考えられる。
そこで本研究では、過去の株価データに対して時系列モデルを適用し、その特性を明らかにするとともに、将来の値を予測することを目的とする。

本論文の構成としては、まず
第2章で取得した時系列データの図示等を行い、データが持つ大まかな特徴について検証し、その上で分析の方針を決定する。
第3章では、RのTIMSACパッケージに含まれるdecomp関数を用いた時系列分解を行い、データの構成要素を明らかにする。
第4章では、ARIMAモデルをデータに適合させ、最適なモデルの選択を行う。そして、構築したモデルを用いて将来の株価予測を行う。
最後に第5章で、本研究で得られた知見を総括し、今後の課題について述べる。


\chapter{データに関する検証} %<--ここに章のタイトルを入れる
この章では、2015年から2024年までの過去10年分の日経平均株価のデータを取得し、時系列データの図示を行うことによって時系列データの
大まかな特徴を捉え、どのような解析を行うべきか方針をたてる。
\section{データの取得} % <--ここに節のタイトルを入れる
今回の分析ではRのquantmodパッケージを使って下記のように記述することで、2015年から2024年の日経平均株価のデータを取得している。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> nikkei <- getSymbols(Symbols = "^N225", 
                       src = "yahoo",
                       from = "2015-01-01",
                       to = "2024-12-31",
                       auto.assign = FALSE)
\end{Verbatim}

\section{データの前処理} \label{sec:maesyori}
分析を行うためのデータの前処理を行う。まずはデータの列構造などについてhead関数を用いて確認する。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> head(nikkei)
           N225.Open N225.High N225.Low N225.Close N225.Volume N225.Adjusted
2015-01-05  17325.68  17540.92 17219.22   17408.71   116500000      17408.71
2015-01-06  17101.58  17111.36 16881.73   16883.19   166000000      16883.19
2015-01-07  16808.26  16974.61 16808.26   16885.33   138600000      16885.33
2015-01-08  17067.40  17243.71 17016.09   17167.10   140600000      17167.10
2015-01-09  17318.74  17342.65 17129.53   17197.73   155200000      17197.73
2015-01-13  16970.88  17087.71 16828.27   17087.71   149200000      17087.71
\end{Verbatim}

このデータには日付に加え、「N225.Open,N225.High,N225.Low,N225.Close,N225.Volume,\\N225.Adjusted」という列が存在し、それぞれ
「始値、高値、安値、終値、出来高、調整後終値」を意味する。今回の分析では、終値が当日の市場取引が全て完結した時点の値であるという観点から、
終値を分析対象とする。次に終値の列だけを抽出し、summary関数を用いてデータの概要を確認する。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> nikkei_cl <- Cl(nikkei)
> summary(nikkei_cl)
     Index              N225.Close   
 Min.   :2015-01-05   Min.   :14952  
 1st Qu.:2017-07-07   1st Qu.:20013  
 Median :2019-12-14   Median :22823  
 Mean   :2019-12-27   Mean   :24832  
 3rd Qu.:2022-06-28   3rd Qu.:28451  
 Max.   :2024-12-30   Max.   :42224  
                      NA's   :21           
\end{Verbatim}

上記の結果から、今回のデータでは終値は14952円から42224円の間で推移していることがわかった。
また、データが21個の欠測値を持つことがわかったため、これらの欠測値について検証する。

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> na <- nikkei_cl[is.na(nikkei_cl)]
> na
           N225.Close
2017-07-17         NA
2017-08-11         NA
2017-09-18         NA
2017-10-09         NA
2017-11-03         NA
2017-11-23         NA
2018-01-01         NA
2018-01-02         NA
2018-01-03         NA
2018-01-08         NA
2018-02-12         NA
2018-03-21         NA
2018-04-30         NA
2018-05-03         NA
2018-05-04         NA
2018-09-17         NA
2018-09-24         NA
2018-10-08         NA
2018-11-23         NA
2018-12-24         NA
2018-12-31         NA
> nikkei_cl <- na.omit(nikkei_cl)
\end{Verbatim}

上記の結果をもとに、欠測値が記録されている日付について調べると、全て祝日であることがわかった。
東京証券取引所では祝日に取引は行われていないため、これらの欠測値を含んだ列は分析に不要と考え、データから取り除く処理を行った。

\section{時系列データの図示} \label{sec:plot}
\ref{sec:maesyori}節で前処理を行った時系列データを用い、観測地点を横軸に、観測値を縦軸にとり隣り合う観測値を直線で結んだ図を描く。
この図を描くことで、時系列データが\emph{トレンド}や\emph{季節性}を含むかどうかを可視化することができる。なお、トレンドと季節性について定義を
\cite{Hyndman}から引用する。
\begin{itemize}
    \item トレンド
    \begin{itemize}
    \item A trend exists when there is a long-term increase or decrease in the data.
     It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, 
     when it might go from an increasing trend to a decreasing trend. 
    \end{itemize}
\end{itemize}    

\begin{itemize}
    \item 季節性
    \begin{itemize}
    \item A seasonal pattern occurs when a time series is affected by seasonal factors such as 
    the time of the year or the day of the week. 
    Seasonality is always of a fixed and known period. 
    \end{itemize}
\end{itemize}   

これらを踏まえた上で、時系列データの図示を行ったところ、図 \ref{fig:nikkei-cl} のような結果が得られた。
この図からこのデータは上昇トレンドがあることが見て取れる。また、季節性に関しては図を見る限り、確認されなかった。
さらに、値が急激に変化している地点について調査を行った。2020年の2月から3月にかけて値の急激な減少の後に、急上昇をしている地点が存在する。
この原因はコロナによる経済活動の停滞及び、それを受けた日銀の金融政策に対する期待によるものであったと考えられる。また、2024年の1月頃に値が急上昇している地点がある。
この原因はNVIDIAが市場を上回る好決算を発表したことで、東京市場の半導体関連株が買われたことによるものであった。
なお、これらは\cite{nikkei-1},\cite{nikkei-2}を参照している。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/nikkei_plot}
\caption{日経平均株価(終値)の推移} 
\label{fig:nikkei-cl} 
\end{figure}

\section{時系列データの自己相関}
自己相関係数とは時系列データの値と一定期間ずらした過去の値との相関関係のことである。ここで、$r_1$を$y$と$y_t$の自己相関係数とするとき、
$r_k$は下記のように書くことができる。

\begin{equation} \label{model:acf}
    r_k = \frac{\sum\limits_{t=k+1}^{T} (y_t - \bar{y})(y_{t-k} - \bar{y})}{\sum\limits_{t=1}^{T} (y_t - \bar{y})^2}
\end{equation}


なお、$T$は時系列データの長さである。そして、異なる$k$についての自己相関係数をまとめたものが、\textbf{自己相関関数(ACF)}である。
このACFをプロットすることによって、データの持つトレンド、及び季節性について確認することができる。
データにトレンドがある場合、観測時点が近い観測値はその値も近いため、小さいラグでの自己相関係数は大きな正値になる可能性が高い。
つまり、トレンドのある時系列データのACFは正値を示す傾向があり、その値はラグが増えるにつれて小さくなる。
また、データに季節性がある場合、季節周期の倍数の箇所で他のラグ次数と比べて、自己相関係数が大きくなる。
これらを踏まえ、取得した時系列データのACFプロットを描いた。
図 \ref{fig:acf} は、データのACFプロットである。また、詳細な自己相関係数についても確認を行った。

\begin{figure}[htbp] %何故か図がコードブロックに入り込んでる
    \centering
    \includegraphics[width=0.9\linewidth]{figures/acf}
    \caption{日経平均株価(終値)の自己相関} 
    \label{fig:acf} 
    \end{figure}

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> acf(nikkei_cl, plot = FALSE)
Autocorrelations of series ‘nikkei_cl’, by lag

    0     1     2     3     4     5     6     7     8     9    10    11    12 
1.000 0.997 0.994 0.992 0.989 0.986 0.984 0.981 0.979 0.976 0.974 0.972 0.969 
   13    14    15    16    17    18    19    20    21    22    23    24    25 
0.967 0.964 0.962 0.960 0.957 0.955 0.953 0.951 0.949 0.947 0.945 0.943 0.941 
   26    27    28    29    30    31    32    33 
0.939 0.937 0.935 0.933 0.931 0.929 0.927 0.924 
\end{Verbatim}

上記の結果から、取得したデータは非常に強い自己相関を持っていることがわかった。また、上述のACFプロットの特性に照らし合わせると、
トレンドを持っている可能性はあり、季節性に関しては持っていないということも確認できた。

\chapter{時系列データの分解}

\section{decomp関数の概要} \label{sec:decomp}
この章では\ref{sec:plot}節で得られた、データが上昇トレンドを持っているという考察を、Rのtimsacパッケージに含まれるdecomp関数を使って
検証する。decomp関数とは時系列データを下記の\ref{model:decomp}式に当てはめ、トレンド成分や季節成分などのいくつかの成分に分解する関数である。

\begin{equation} \label{model:decomp}
y(t) = T(t) + AR(t) + S(t) + TD(t) + W(t)
\end{equation} 

ここで、T(t)はトレンド成分、AR(t)はAR成分、S(t)は季節成分、TD(t)は曜日効果成分、W(t)は観測ノイズである。
なお, これらは \cite{timsac} を参照している。\\
decomp関数を用いるためには各成分の次数を適切に設定し、当てはまりの良いモデルを作成する必要がある。
よって、次にモデルの次数選択を行う。まず、図\ref{fig:nikkei-cl}及び図\ref{fig:acf}から今回のデータは季節成分及び、曜日効果成分を持たないと考え、除外する。
次にトレンド成分、AR成分の次数を決定するために、AIC(赤池情報量基準, Akaike's Information Criterion)を用いる。
トレンド成分の次数を1,2,3、AR成分の次数を1--7に設定し、全ての組み合わせのAICを求め、
これらの組み合わせの中で最もAICが小さいモデルを採用し、decomp関数を実行する。

\newpage

\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
    trend.order = 1 ar.order = 1 seasonal.order = 0 AIC = 35465.07 
    trend.order = 1 ar.order = 2 seasonal.order = 0 AIC = 35370.69 
    trend.order = 1 ar.order = 3 seasonal.order = 0 AIC = 35369.54 
    trend.order = 1 ar.order = 4 seasonal.order = 0 AIC = 35368.89 
    trend.order = 1 ar.order = 5 seasonal.order = 0 AIC = 35631.88 
    trend.order = 1 ar.order = 6 seasonal.order = 0 AIC = 35354.22 
    trend.order = 1 ar.order = 7 seasonal.order = 0 AIC = 35388.21 
    trend.order = 2 ar.order = 1 seasonal.order = 0 AIC = 35644.09 
    trend.order = 2 ar.order = 2 seasonal.order = 0 AIC = 35411.51 
    trend.order = 2 ar.order = 3 seasonal.order = 0 AIC = 35402.68 
    trend.order = 2 ar.order = 4 seasonal.order = 0 AIC = 35389.37 
    trend.order = 2 ar.order = 5 seasonal.order = 0 AIC = 35393.69 
    trend.order = 2 ar.order = 6 seasonal.order = 0 AIC = 35390.23 
    trend.order = 2 ar.order = 7 seasonal.order = 0 AIC = 35391.92 
    trend.order = 3 ar.order = 1 seasonal.order = 0 AIC = 35832.37 
    trend.order = 3 ar.order = 2 seasonal.order = 0 AIC = 35560.81 
    trend.order = 3 ar.order = 3 seasonal.order = 0 AIC = 35550.06 
    trend.order = 3 ar.order = 4 seasonal.order = 0 AIC = 35530.52 
    trend.order = 3 ar.order = 5 seasonal.order = 0 AIC = 35534.47 
    trend.order = 3 ar.order = 6 seasonal.order = 0 AIC = 35527.38 
    trend.order = 3 ar.order = 7 seasonal.order = 0 AIC = 35530.68    
\end{Verbatim}
上記の結果から、トレンド次数 = 1, AR次数 = 6の組み合わせがAIC = 35354.22で最小になった。よって、このモデルを採用し、decomp
関数を実行する。

\newpage

\section{decomp関数の実行} \label{sec:decomp-result}

\ref{sec:decomp}節で決定したモデルを用いて、下記のように記述し、時系列データの分解を行う。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> dp <- decomp(nikkei_cl, trend.order = 1, ar.order = 6, seasonal.order = 0)
\end{Verbatim}
上記を実行すると図\ref{fig:decomp}のような結果が得られた。この結果から、時系列データがトレンド成分を持つことが明らかになった。
また、ノイズに関してもほとんど見られないためモデルの当てはまりは良いと考えられる。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/decomp_plot.png}
    \caption{時系列データの分解} 
    \label{fig:decomp} 
    \end{figure}



\chapter{ARIMAモデルによる予測}
\section{定常性の確認} \label{sec:teizyou}
この章では過去10年分の日経平均株価の時系列データを用いて、今後の変動を予測する。予測を行うには時系列データの特徴を踏まえ、適切な時系列モデルを選択
する必要がある。時系列モデルの選択を行う上で重要なのは、時系列データの\emph{定常性}を確認することである。ここで、定常性について定義を行う。\\
$l$を時間のシフト量を表す任意の整数とするとき
\begin{align}
\begin{split} 
& E[y_n] = E[y_{n-l}] \\
& Var(y_n) = Var(y_{n-l}) \\
& Cov(y_n,y_m) = Cov(y_{n-l}, y_{m-l}) \\
\end{split}
\end{align}
が成り立つ時系列は\emph{弱定常}であると呼ばれる。
また、時系列の分布が時間のシフトに関して不変で、その確率分布を時間軸方向に移動しても変化しないとき、その時系列は\emph{強定常}と呼ばれる。
なお、これらは\cite{Kitagawa}を参照している。
一般的に弱定常性をもつ時系列のことを定常な時系列と呼び、本研究での定常は弱定常のことを指す。また、上記の定義に当てはまらない定常ではない時系列を
非定常な時系列と呼ぶ。今回使用する時系列データは、\ref{sec:decomp-result}節で上昇トレンドを持っていることがわかっており、時点が異なると
値が異なるため非定常の時系列データである。

\newpage

\section{ARIMAモデルの概要}
非定常な時系列データに対しては様々な分析手法が存在するが、今回はARIMAモデル
を使用し、将来の値を予測する。ここで、ARIMAモデルについての定義を行う。そのために必要な\emph{差分}、
\emph{自己回帰モデル}、\emph{移動平均モデル}について説明する。なお, これらは \cite{Hyndman} を参照している。

\subsection{差分}
非定常な時系列を定常にする方法の1つに、連続する観測値の差分を計算する方法がある。これを差分を取ると言う。差分を取ることは
時系列のトレンドや季節性を除去、あるいは低減し、時系列の平均を安定化させることに役立つ。
ここで、差分系列(The differenced series)は元の系列の連続する観測値間の変化であり下記のように書くことができる。
\begin{equation} \label{model:diff}
y'_t = y_t - y_{t-1}
\end{equation} 
また、一度差分を取っても時系列が定常にならないときは2次差分を取る必要がある場合も存在する。
\begin{align}
\begin{split} \label{model:doff-second}
    y''_t &= y'_t - y'_{t-1} \\
          &= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) \\
          &= y_t - 2y_{t-1} + y_{t-2}
\end{split}
\end{align}
なお、実際の分析では2次よりも高次の差分を取ることは必要ない場合が多い。

\subsection{自己回帰モデル}
下記のようなモデルを$p$次の自己回帰モデル(autoregressive model)、$AR(p)$モデルと呼ぶ。
\begin{equation} \label{model:ar}
y_t = c + \phi_1y_{t-1} + \phi_2y_{t-2} + \dots + \phi_py_{t-p} + \varepsilon_t
\end{equation}
ただし、$\varepsilon_t$はホワイトノイズである。

\subsection{移動平均モデル}
下記のようなモデルを$q$次の移動平均モデル(moving average model)、$MA(q)$モデルと呼ぶ。
\begin{equation} \label{model:ma}
y_t = c + \varepsilon_t +\theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q} 
\end{equation}
ただし、$\varepsilon_t$はホワイトノイズである。
\subsection{ARIMAモデル}
差分化、自己回帰モデル、移動平均モデルを組み合わせると、ARIMAモデル(AutoRegressive Integrated Moving Average)
となる。ARIMAモデルのモデル式は下記のように書くことができる。
\begin{equation} \label{model:arima}
y'_t = c + \phi_1y'_{t-1} + \dots + \phi_py'_{t-p} + \theta_1\varepsilon_{t-1} + \dots + \theta_q\varepsilon_{t-q} + \varepsilon_t
\end{equation} 
ただし$y'_t$は一回を超えて差分を取る場合もある。\ref{model:arima}式を$ARIMA(p,d,q)$モデルと呼ぶ。

\section{モデル化の手順} \label{sec:model}
次に、実際に日経平均株価の時系列データに対してARIMAモデルを当てはめていく。
時系列データにARIMAモデルを適合する際は、下記の手順を実施する。
\begin{enumerate}[1.]
    \item データをプロットして、異常値を特定する。
    \item 必要なら、(Box-Cox変換を使って)データを変換し、分散を安定化させる。
    \item データが非定常なら、定常になるまでデータの1階差分を取る。
    \item ACF（自己相関関数）とPACF（偏自己相関関数）を調べ、ARIMA(p,d,0)モデル、またはARIMA(0,d,q)モデルが適切かどうか検討する。
    \item 選んだモデルを試す、そして、AICcを使ってより良いモデルを探索する。
    \item 選んだモデルの残差を、ACFプロットやportmanteau検定でチェックする。残差がホワイトノイズらしくなければ、モデルを修正して試す。
    \item 残差がホワイトノイズらしく見えるようになったら、予測を計算する。
\end{enumerate}
なお, これらは \cite{Hyndman} を参照している。

\section{ARIMA()関数について} \label{sec:ARIMA()}
モデル化を行うにあたって、\ref{sec:model}節で示した、3〜5のステップをfableパッケージのARIMA()関数を用いて実施する。ARIMA()関数は単位根検定とAICc最小化、そして最尤法を組み合わせて
ARIMAモデルを得る\textbf{Hyndman-Khandakarアルゴリズム}を使用している。
なお、Hyndman-Khandakarアルゴリズムについては\cite{Hyndman-and-Khandakar}を参照されたい。
ARIMA()関数は引数を指定することで、様々なアルゴリズムの変形を用いることができる。下記にHyndman-Khandakarアルゴリズムに従った、デフォルトでの
動作を記述する。
\begin{enumerate}[1.]
    \item 差分の次数 $0 \le d \le 2$ をKPSS検定を繰り返して決める。
    \item 次に、$d$ 回差分を取ったデータでAICcを最小化して、$p$ と $q$ の値を選ぶ。アルゴリズムは $p$ と $q$ の全ての可能な組み合わせを考えるのではなく、ステップワイズ探索を使ってモデル空間を探索する。
    \begin{enumerate}[a.]
        \item 最初に下記の4つのモデルに適合させる。
        \begin{itemize}
            \item $ARIMA(0,d,0)$
            \item $ARIMA(2,d,2)$
            \item $ARIMA(1,d,0)$
            \item $ARIMA(0,d,1)$
        \end{itemize}
        $d = 2$ の場合を除き、定数項が含まれる。$d \le 1$ なら、定数項なしの $ARIMA(0,d,0)$も追加で当てはめられる。
        
        \item (a)のステップで適合させた中で（最小のAICc値を持つ）最良モデルを「現在のモデル」とする。
        
        \item 現在のモデルの変形を以下のようにして考える。
        \begin{itemize}
            \item 現在のモデルから、$\pm 1$ だけ $p$ と $q$ の両方か一方を変えてみる。
            \item 現在のモデルから、$c$ を取り除いたり、含めたりしてみる。
        \end{itemize}
        ここまでで最良と考えられるモデル（現在のモデルかこれらの変形の1つ）が新しい現在のモデルになる。
        
        \item (c)のステップをAICcがそれ以上小さくならなくなるまで繰り返す。
    \end{enumerate}
\end{enumerate}

\section{モデル作成} \label{sec:make-model}
\ref{sec:ARIMA()}を踏まえて、ARIMA()関数を実行し、モデル化を行う。Rで下記のように記述することでモデル化を行うことができる。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
    > nikkei_ts <- tibble(
+   date  = as.Date(time(nikkei_cl)),       
+   close = as.numeric(nikkei_cl)         
+ ) |> 
+   as_tsibble(index = date)
> 
> nikkei_ts_idx <- nikkei_ts |>
+   mutate(t = row_number()) |>
+   as_tsibble(index = t)
> 
> model <- nikkei_ts_idx |>
+   model(auto = ARIMA(close))
> 
> 
> report(model)
Series: close 
Model: ARIMA(2,1,1) w/ drift 

Coefficients:
          ar1     ar2     ma1  constant
      -0.7151  0.0121  0.6841   15.5509
s.e.   0.1616  0.0236  0.1605   11.1745

sigma^2 estimated as 107779:  log likelihood=-17626.22
AIC=35262.44   AICc=35262.47   BIC=35291.45
\end{Verbatim}
上記の結果から、アルゴリズムにより、$ARIMA(2,1,1)$モデルが選ばれたことがわかった。\ref{sec:model}節の手順6に従い、次節でこのモデルについての検証を行う。

\newpage

\section{モデルの評価} \label{sec:model-check}
\ref{sec:model}節より、モデルの残差をACFプロット、及びportmanteau検定で確認する。

\subsection{残差のACFプロット}
Rで下記のように記述することで、残差のACFプロットを作成した。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
model |>
  gg_tsresiduals()
\end{Verbatim}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/acf_residual.png}
\caption{残差のACFプロット} 
\label{fig:acf-residual} 
\end{figure}

図\ref{fig:acf-residual}から残差は概ね、信頼区間に収まっているが、ラグ20及び25で信頼区間を飛び出している。よってこのラグまでの期間でLjung-Box検定
を行い、予測に問題がないか確認する。

\newpage

\subsection{portmanteau検定}
Rで下記のように記述することで、portmanteau検定の一種であるLjung-Box検定を行った。なお、portmanteau検定及び、Ljung-Box検定については
\cite{Ljung}を参照されたい。
\begin{Verbatim}[fontsize=\small, baselinestretch=0.9, frame=single]
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 10, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      8.36     0.302
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 20, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      25.2    0.0893
> model |>
+   augment() |>
+   filter(.model == "auto") |>
+   features(.innov, ljung_box, lag = 25, dof = 3)
# A tibble: 1 × 3
  .model lb_stat lb_pvalue
  <chr>    <dbl>     <dbl>
1 auto      30.2     0.115

\end{Verbatim}
上記の結果から、ラグが10,20,25の場合でいずれもp値は0.05よりも大きいため予測を実施するには問題ないということがわかる。

\newpage

\section{予測と考察}
\ref{sec:model-check}節でモデルの残差がホワイトノイズであることが確かめられたため、\ref{sec:model}節の手順7に移行し、将来の値の予測を行う。
なお、今回は1年分の予測を行うことを目標にし、東京証券取引所の1年間の営業日数約250日分の予測を行う。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\linewidth]{figures/predict_plot.png}
\caption{日経平均株価(終値)の予測プロット} 
\label{fig:predict} 
\end{figure}
図\ref{fig:predict}から、日経平均株価は将来に渡って、上昇傾向にあることが予測される。
また、実際に日経平均株価は2025年12月の時点で5万円台を推移しており、今回の予測は実際の株価の推移と概ね一致している。

\chapter{おわりに} % <--ここに章のタイトルを入れる

本研究では、日経平均株価の時系列データを分析し、データの持つ統計的特徴を明らかにすること、および時系列モデルを作成して将来の値を予測することを目的とした。
第2章では時系列データの図示により大まかな特徴を捉え、第3章ではdecomp関数による時系列分解、第4章ではARIMAモデルを用いた将来予測を行った。
分析の結果、対象期間におけるデータは明確な上昇トレンドを持つこと、またモデル選択においては$ARIMA(2,1,1)$モデルが最適であること、そして1年間の予測では日経平均株価が5万円台に到達する可能性があることの3点が明らかとなった。

しかしながら、本研究には限界も存在する。今回は単変量のARIMAモデルを使用したが、株価などの複雑な金融指標を過去の自身の値のみで完全に説明することは困難である。
実際の市場は、国内外の目まぐるしく変化する政治情勢や金融政策、あるいはAIをはじめとした技術革新によるビジネス環境の急速な変化など、多種多様な外的要因の影響を強く受ける。
例えば、2025年における株価の急変動に見られるように、事前のモデル予測を覆すような社会的・政治的イベントが発生した場合、単変量モデルでの対応には限界がある。

したがって、より精緻な予測を行うためには、過去の価格データだけでなく、経済指標やテキストデータ（ニュースや要人発言）などの外部変数を組み込んだ多変量モデルの構築などが求められるだろう。
今後は、こうした技術的な知見を深めることに加え、経済に影響を与えうる社会情勢を継続的に注視し、分析モデルへと反映させていくことを課題としたい。

\begin{thebibliography}{99} % 文献リストの作成

\bibitem{Hyndman} 
Hyndman, R.J., \& Athanasopoulos, G. (2021)
\textit{Forecasting: principles and practice, 3rd edition},
OTexts: Melbourne, Australia. 
URL: \url{https://otexts.com/fpp3/}. 
Accessed on 2 December 2025

\bibitem{timsac}
The Institute of Statistical Mathematics (2023)
\textit{timsac: Time Series Analysis and Control Package},
URL: \url{https://cran.r-project.org/package=timsac}.



\bibitem{Kitagawa} 北川源四郎 (2020) 
『Rによる時系列モデリング入門』,
岩波書店

\bibitem{nikkei-1} 
日本経済新聞, (10 April 2020),
『日経平均、3月は記録ずくめ新型コロナで変動大きく: 株式投資の超キホン日経平均を知ろう!(17)』,
URL: \url{https://www.nikkei.com/article/DGXMZO57807450Y0A400C2I00000/}

\bibitem{nikkei-2} 
日本経済新聞, (22 February 2024),
『日経平均、終値3万9098円　34年ぶり最高値更新』,
URL: \url{https://www.nikkei.com/article/DGXZQOFL220S70S4A220C2000000/}

\bibitem{Hyndman-and-Khandakar}
Hyndman, R. J., \& Khandakar, Y., (2008)
『Automatic time series forecasting: The forecast package for R. Journal of Statistical Software』,
URL: \url{https://www.jstatsoft.org/article/view/v027i03}

\bibitem{Ljung} G. M. Ljung and G. E. P. Box (1978)
『On a Measure of Lack of Fit in Time Series Models』, 
Biometrika, Vol.65, No.2, pp. 297-303, 
Oxford University Press. 

\end{thebibliography}


\end{document}